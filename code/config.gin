from __gin__ import dynamic_registration
import __main__ as train_script
from mt3 import mixing
from mt3 import models
from mt3 import network
from mt3 import preprocessors
from mt3 import spectrograms
from mt3 import vocabularies
import seqio
from t5x import adafactor
from t5x import gin_utils
from t5x import partitioning
from t5x import trainer
from t5x import utils
import tasks

# Macros:
# ==============================================================================
BATCH_SIZE = 32
CHECKPOINT_PATH = '/home/marvin/US/TFM/code/checkpoints/checkpoint_1'
EVAL_STEPS = 20
EVAL_TASK_SUFFIX = 'eval'
EVALUATOR_NUM_EXAMPLES = None
EVALUATOR_USE_MEMORY_CACHE = True
JSON_WRITE_N_RESULTS = 0
LABEL_SMOOTHING = 0.0
LOSS_NORMALIZING_FACTOR = None
MAX_EXAMPLES_PER_MIX = None
MODEL = @models.ContinuousInputsEncoderDecoderModel()
MODEL_DIR = '/home/marvin/US/TFM/code'
NUM_VELOCITY_BINS = 1
ONSETS_ONLY = False
OPTIMIZER = @adafactor.Adafactor()
OUTPUT_VOCABULARY = @vocabularies.vocabulary_from_codec()
PROGRAM_GRANULARITY = 'full'
SPECTROGRAM_CONFIG = @spectrograms.SpectrogramConfig()
TASK_FEATURE_LENGTHS = {'inputs': 256, 'targets': 2048}
TASK_PREFIX = 'jazz_solos_notes'
TRAIN_STEPS = 15000
TRAIN_TASK_SUFFIX = 'train'
USE_CACHED_TASKS = False
USE_TIES = True
VOCAB_CONFIG = @vocabularies.VocabularyConfig()
Z_LOSS = 0.0001

# Parameters for adafactor.Adafactor:
# ==============================================================================
adafactor.Adafactor.decay_rate = 0.8
adafactor.Adafactor.logical_factor_rules = \
    @adafactor.standard_logical_factor_rules()
adafactor.Adafactor.step_offset = 0

# Parameters for vocabularies.build_codec:
# ==============================================================================
vocabularies.build_codec.vocab_config = %VOCAB_CONFIG

# Parameters for utils.CheckpointConfig:
# ==============================================================================
utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()

# Parameters for eval/tasks.construct_task_name:
# ==============================================================================
eval/tasks.construct_task_name.task_prefix = %TASK_PREFIX
eval/tasks.construct_task_name.task_suffix = %EVAL_TASK_SUFFIX
eval/tasks.construct_task_name.vocab_config = %VOCAB_CONFIG

# Parameters for train/tasks.construct_task_name:
# ==============================================================================
train/tasks.construct_task_name.task_prefix = %TASK_PREFIX
train/tasks.construct_task_name.task_suffix = %TRAIN_TASK_SUFFIX
train/tasks.construct_task_name.vocab_config = %VOCAB_CONFIG

# Parameters for models.ContinuousInputsEncoderDecoderModel:
# ==============================================================================
models.ContinuousInputsEncoderDecoderModel.input_depth = \
    @spectrograms.input_depth()
models.ContinuousInputsEncoderDecoderModel.input_vocabulary = \
    @seqio.vocabularies.PassThroughVocabulary()
models.ContinuousInputsEncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING
models.ContinuousInputsEncoderDecoderModel.loss_normalizing_factor = \
    %LOSS_NORMALIZING_FACTOR
models.ContinuousInputsEncoderDecoderModel.module = @network.Transformer()
models.ContinuousInputsEncoderDecoderModel.optimizer_def = %OPTIMIZER
models.ContinuousInputsEncoderDecoderModel.output_vocabulary = %OUTPUT_VOCABULARY
models.ContinuousInputsEncoderDecoderModel.z_loss = %Z_LOSS

# Parameters for utils.create_learning_rate_scheduler:
# ==============================================================================
utils.create_learning_rate_scheduler.base_learning_rate = 0.001
utils.create_learning_rate_scheduler.factors = 'constant'
utils.create_learning_rate_scheduler.warmup_steps = 1000

# Parameters for infer_eval/utils.DatasetConfig:
# ==============================================================================
infer_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE
infer_eval/utils.DatasetConfig.mixture_or_task_name = \
    @eval/tasks.construct_task_name()
infer_eval/utils.DatasetConfig.pack = False
infer_eval/utils.DatasetConfig.seed = 42
infer_eval/utils.DatasetConfig.shuffle = False
infer_eval/utils.DatasetConfig.split = 'eval'
infer_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
infer_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS

# Parameters for train/utils.DatasetConfig:
# ==============================================================================
train/utils.DatasetConfig.batch_size = %BATCH_SIZE
train/utils.DatasetConfig.mixture_or_task_name = @train/tasks.construct_task_name()
train/utils.DatasetConfig.pack = False
train/utils.DatasetConfig.seed = None
train/utils.DatasetConfig.shuffle = True
train/utils.DatasetConfig.split = 'train'
train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS

# Parameters for train_eval/utils.DatasetConfig:
# ==============================================================================
train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE
train_eval/utils.DatasetConfig.mixture_or_task_name = \
    @train/tasks.construct_task_name()
train_eval/utils.DatasetConfig.pack = False
train_eval/utils.DatasetConfig.seed = 42
train_eval/utils.DatasetConfig.shuffle = False
train_eval/utils.DatasetConfig.split = 'eval'
train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS

# Parameters for seqio.Evaluator:
# ==============================================================================
seqio.Evaluator.logger_cls = \
    [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]
seqio.Evaluator.num_examples = %EVALUATOR_NUM_EXAMPLES
seqio.Evaluator.use_memory_cache = %EVALUATOR_USE_MEMORY_CACHE

# Parameters for spectrograms.input_depth:
# ==============================================================================
spectrograms.input_depth.spectrogram_config = %SPECTROGRAM_CONFIG

# Parameters for seqio.JSONLogger:
# ==============================================================================
seqio.JSONLogger.write_n_results = %JSON_WRITE_N_RESULTS

# Parameters for preprocessors.map_midi_programs:
# ==============================================================================
preprocessors.map_midi_programs.granularity_type = %PROGRAM_GRANULARITY

# Parameters for mixing.mix_transcription_examples:
# ==============================================================================
mixing.mix_transcription_examples.max_examples_per_mix = %MAX_EXAMPLES_PER_MIX

# Parameters for vocabularies.num_embeddings:
# ==============================================================================
vocabularies.num_embeddings.vocabulary = %OUTPUT_VOCABULARY

# Parameters for seqio.vocabularies.PassThroughVocabulary:
# ==============================================================================
seqio.vocabularies.PassThroughVocabulary.size = 0

# Parameters for partitioning.PjitPartitioner:
# ==============================================================================
partitioning.PjitPartitioner.model_parallel_submesh = None
partitioning.PjitPartitioner.num_partitions = 1

# Parameters for utils.RestoreCheckpointConfig:
# ==============================================================================
utils.RestoreCheckpointConfig.mode = 'specific'
utils.RestoreCheckpointConfig.path = %CHECKPOINT_PATH

# Parameters for utils.SaveCheckpointConfig:
# ==============================================================================
utils.SaveCheckpointConfig.dtype = 'float32'
utils.SaveCheckpointConfig.keep = None
utils.SaveCheckpointConfig.period = 5000
utils.SaveCheckpointConfig.save_dataset = False

# Parameters for network.T5Config:
# ==============================================================================
network.T5Config.dropout_rate = 0.1
network.T5Config.dtype = 'float32'
network.T5Config.emb_dim = 768
network.T5Config.head_dim = 64
network.T5Config.logits_via_embedding = False
network.T5Config.mlp_activations = ('gelu', 'linear')
network.T5Config.mlp_dim = 2048
network.T5Config.num_decoder_layers = 12
network.T5Config.num_encoder_layers = 12
network.T5Config.num_heads = 12
network.T5Config.vocab_size = @vocabularies.num_embeddings()

# Parameters for train_script.train:
# ==============================================================================
train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
train_script.train.eval_period = 5000
train_script.train.eval_steps = %EVAL_STEPS
train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()
train_script.train.inference_evaluator_cls = @seqio.Evaluator
train_script.train.model = %MODEL
train_script.train.model_dir = %MODEL_DIR
train_script.train.partitioner = @partitioning.PjitPartitioner()
train_script.train.random_seed = None
train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
train_script.train.total_steps = %TRAIN_STEPS
train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()
train_script.train.trainer_cls = @trainer.Trainer

# Parameters for trainer.Trainer:
# ==============================================================================
trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
trainer.Trainer.num_microbatches = None

# Parameters for network.Transformer:
# ==============================================================================
network.Transformer.config = @network.T5Config()

# Parameters for vocabularies.vocabulary_from_codec:
# ==============================================================================
vocabularies.vocabulary_from_codec.codec = @vocabularies.build_codec()

# Parameters for vocabularies.VocabularyConfig:
# ==============================================================================
vocabularies.VocabularyConfig.num_velocity_bins = %NUM_VELOCITY_BINS
